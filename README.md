# CIL ETH ProjectğŸ§ª


This repo is for all our experiments related to the CIL project at ETH. It's structured to make it easy to plug in new models, configs, datasets, etc.

---

## ğŸ—‚ Repo Structure

### configs/ âš™ï¸  
Contains training setup:
- Hyperparameters (learning rate, batch size, etc.)
- Augmentations
- Model/optimizer/loss initialization

### datasets/ ğŸ“š  
PyTorch Dataset class for defining (train_x, label_y) pairs.  
Originally taken from the baseline â€“ works fine as-is.

### models/ ğŸ§   
Model architectures:
- Both custom and baseline versions
- Naming convention:
  - <model_name>.py â†’ main architecture
  - <model_name>_utils.py â†’ extra blocks if needed

### utils/ ğŸ›   
Currently has train_utils.py with train/validation/test loops used in notebooks. You may also add here any additional supplementary functions.

### notebooks/ ğŸ““  
Where experiments are run:
- Import configs, split data, train/evaluate models
- Includes a notebook to download data (requires Kaggle API key)

---

## âš ï¸ Before You Run Anything

- Make sure to set the correct data path and preferred GPU in configs or notebooks.
- For the data download notebook, your Kaggle API key is needed (get it from your [Kaggle account](https://www.kaggle.com/settings)).

---

## ğŸ’¡ Potential Improvements

Some areas we could explore next:

- Augmentations: Right now itâ€™s just the baseline setup. Could try things like flipping, perspective shifts, etc. Just keep in mind the target/label needs to be transformed too.
- Model architectures: Only one custom model so far. Lots of other designs to test out or build on top of.
- Losses/optimizers: Using AdamW + MSE for now. Since our metric is scale-invariant RMSE, maybe MSE isnâ€™t ideal. Might be worth trying alternatives or optimizing the metric directly.

---

## âœ… To-Do / Wishlist
- [ ] Add more advanced augmentations
- [ ] Try different model variants
- [ ] Run ablation studies on loss functions, optimizers, and hyperparameters

---

Hope that helps!
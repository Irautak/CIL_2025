{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b62ca78",
   "metadata": {},
   "source": [
    "# Testing the combination model using a CNN for the fusion of the pretrained depth maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09fe1028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucijatonkovic/miniforge3/envs/machine_perception/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/lucijatonkovic/miniforge3/envs/machine_perception/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/lucijatonkovic/miniforge3/envs/machine_perception/lib/python3.12/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.6'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import torch, wandb\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "sys.path.append(os.path.abspath(os.path.join(os.curdir, '..')))\n",
    "from configs import combination_model_config as config\n",
    "from models.unet_convnextv2 import Unet\n",
    "from models.fusion_models import CNNFusionModel\n",
    "from models.combination_model import CombinedModel\n",
    "from datasets.combination_depth_dataset import CombDepthDataset\n",
    "from utils.train_utils import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fc690a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f815bfcacf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc4be6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] idx=21744, stacked_depths=[torch.Size([448, 576]), torch.Size([448, 576]), torch.Size([448, 576]), torch.Size([448, 576])]\n",
      "[DEBUG] idx=9954, stacked_depths=[torch.Size([448, 576]), torch.Size([448, 576]), torch.Size([448, 576]), torch.Size([448, 576])]\n",
      "Loss: 4.8876\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CombDepthDataset(\n",
    "    data_dir=os.path.join(config.dataset_path, 'train/train'),\n",
    "    depths_dir=os.path.join(config.depth_maps_path, 'train'),\n",
    "    list_file=os.path.join(config.dataset_path, 'train_list.txt'),\n",
    "    transform=config.padded_transform,\n",
    "    target_transform=config.target_transform,\n",
    "    has_gt=True,\n",
    "    depth_model_names=config.depth_model_names,\n",
    "    uncertainty_dir=None,\n",
    "    use_uncertainty=None\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Initialize models\n",
    "fusion_model = CNNFusionModel(input_channels=len(dataset.depth_model_names))\n",
    "unet_model = Unet(features_included=True, uncertainty_included=False)\n",
    "model = CombinedModel(fusion_model=fusion_model, unet_model=unet_model, use_uncertainty=False)\n",
    "\n",
    "# Move to GPU if available\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   =>  did not fit on mine :)\n",
    "device = torch.device('gpu:3')\n",
    "torch.cuda.empty_cache() \n",
    "model = model.to(device)\n",
    "\n",
    "# Dummy loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "#  One forward-backward step \n",
    "model.train()\n",
    "for batch in dataloader:\n",
    "    rgb, depth_stack, gt_depth, filenames, uncertainty_map = batch\n",
    "    rgb = rgb.to(device)\n",
    "    depth_stack = depth_stack.to(device)\n",
    "    gt_depth = gt_depth.to(device)\n",
    "    uncertainty_map = uncertainty_map.to(device) if uncertainty_map is not None else None\n",
    "\n",
    "    output = model(rgb, depth_stack, uncertainty_map)\n",
    "    output = config.unpad_to_original(output, config.img_size)\n",
    "    loss = criterion(output, gt_depth)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    break  # just test one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ca202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
